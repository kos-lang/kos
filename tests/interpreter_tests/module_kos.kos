# SPDX-License-Identifier: MIT
# Copyright (c) 2014-2020 Chris Dragan

import base
import kos

fun expect_fail(function)
{
    var failed = false
    try {
        function()
    }
    catch const e {
        failed = true
        if base.args.size > 1 && base.args[1] == "-print" {
            e.print()
        }
    }
    assert failed
}

do {
    const buf = base.buffer();
    buf.pack("s", "try/*comment*/+0=0x44")
    const expected = [ [  1, "token_keyword",  "try"         ],
                       [  4, "token_comment",  "/*comment*/" ],
                       [ 15, "token_operator", "+"           ],
                       [ 16, "token_numeric",  "0"           ],
                       [ 17, "token_operator", "="           ],
                       [ 18, "token_numeric",  "0x44"        ] ]
    var idx = 0

    for var token in kos.raw_lexer(buf) {
        assert token.line   == 1
        assert token.column == expected[idx][0]
        assert token.type   == kos[expected[idx][1]]
        assert token.token  == expected[idx][2]
        idx += 1

        # Force buffer reallocation in the middle of parsing
        if idx == 3 {
            buf.reserve(1024)
        }
    }

    assert idx == 6
}

do {
    expect_fail(fun {
        const buf = base.buffer()
        buf.pack("s", "'")
        const it = kos.raw_lexer(buf)
        it()
    })
}

do {
    const lexer = base.buffer().pack("s", ")hello\"") -> kos.raw_lexer

    const t1 = lexer()
    assert t1.line   == 1
    assert t1.column == 1
    assert t1.type   == kos.token_separator
    assert t1.sep    == kos.sep_paren_close
    assert t1.token  == ")"

    const t2 = lexer(kos.continue_string)
    assert t2.line   == 1
    assert t2.column == 1
    assert t2.type   == kos.token_string
    assert t2.token  == ")hello\""

    expect_fail(lexer)
}

do {
    const lexer = base.buffer().pack("s", ")str\"") -> kos.raw_lexer
    const t     = lexer(kos.continue_string)

    assert t.type  == kos.token_separator
    assert t.sep   == kos.sep_paren_close
    assert t.token == ")"
}

do {
    const lexer = base.buffer().pack("s", "\"a\\()b\"") -> kos.raw_lexer

    const t1 = lexer()
    assert t1.line   == 1
    assert t1.column == 1
    assert t1.type   == kos.token_string_open
    assert t1.token  == "\"a\\("

    const t2 = lexer(kos.any_token)
    assert t2.line   == 1
    assert t2.column == 5
    assert t2.type   == kos.token_separator
    assert t2.sep    == kos.sep_paren_close
    assert t2.token  == ")"

    const t3 = lexer(kos.continue_string)
    assert t3.line   == 1
    assert t3.column == 5
    assert t3.type   == kos.token_string
    assert t3.token  == ")b\""
}

do {
    const lexer = base.buffer().pack("s", "(str\"") -> kos.raw_lexer

    const t1 = lexer()
    expect_fail(()=>lexer(kos.continue_string))
}

do {
    const lexer = kos.lexer("filename", base.buffer().pack("s", "{([\"\\()\"])}"))

    const expected = [ [  1, "token_separator",   "sep_curly_open",   "{"     ],
                       [  2, "token_separator",   "sep_paren_open",   "("     ],
                       [  3, "token_separator",   "sep_square_open",  "["     ],
                       [  4, "token_string_open", "sep_none",         "\"\\(" ], #)\"
                       [  7, "token_string",      "sep_none",         ")\""   ],
                       [  9, "token_separator",   "sep_square_close", "]"     ],
                       [ 10, "token_separator",   "sep_paren_close",  ")"     ],
                       [ 11, "token_separator",   "sep_curly_close",  "}"     ] ]

    var idx = 0

    for var token in lexer {
        assert token.line   == 1
        assert token.column == expected[idx][0]
        assert token.type   == kos[expected[idx][1]]
        assert token.sep    == kos[expected[idx][2]]
        assert token.token  == expected[idx][3]
        idx += 1
    }

    assert idx == 8
}

do {
    const input = "+\\hello"
    const buf   = base.buffer().pack("s", input)

    do {
        const lexer = kos.raw_lexer(buf)
        const token = lexer()
        assert token.type  == kos.token_operator
        assert token.token == "+"
        assert token.op    == kos.op_add
        expect_fail(lexer)
        expect_fail(lexer)
    }

    const lexer = kos.raw_lexer(buf, true) # ignore_errors=true

    const t1 = lexer()
    assert t1.type   == kos.token_operator
    assert t1.token  == "+"
    assert t1.op     == kos.op_add
    assert t1.line   == 1
    assert t1.column == 1

    const t2 = lexer()
    assert t2.type    == kos.token_whitespace
    assert t2.token   == "\\"
    assert t2.keyword == kos.keyword_none
    assert t2.op      == kos.op_none
    assert t2.sep     == kos.sep_none
    assert t2.line    == 1
    assert t2.column  == 2

    const t3 = lexer()
    assert t3.type   == kos.token_identifier
    assert t3.token  == "hello"
    assert t3.line   == 1
    assert t3.column == 3

    expect_fail(lexer)
}
