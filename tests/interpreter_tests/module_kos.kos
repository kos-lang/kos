#
# Copyright (c) 2014-2017 Chris Dragan
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to
# deal in the Software without restriction, including without limitation the
# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
# sell copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
# IN THE SOFTWARE.
#

import lang
import kos

fun expect_fail(function)
{
    var failed = false
    try {
        function()
    }
    catch const e {
        failed = true
        if lang.args.size > 1 && lang.args[1] == "-print" {
            e.print()
        }
    }
    assert failed
}

do {
    const buf = lang.buffer();
    buf.pack("s", "try/*comment*/+0=0x44")
    const expected = [ [  1, "token_keyword",  "try"         ],
                       [  4, "token_comment",  "/*comment*/" ],
                       [ 15, "token_operator", "+"           ],
                       [ 16, "token_numeric",  "0"           ],
                       [ 17, "token_operator", "="           ],
                       [ 18, "token_numeric",  "0x44"        ] ]
    var idx = 0

    for var token in kos.raw_lexer(buf) {
        assert token.line   == 1
        assert token.column == expected[idx][0]
        assert token.type   == kos[expected[idx][1]]
        assert token.token  == expected[idx][2]
        idx += 1

        # Force buffer reallocation in the middle of parsing
        if idx == 3 {
            buf.reserve(1024)
        }
    }

    assert idx == 6
}

do {
    expect_fail(fun {
        const buf = lang.buffer()
        buf.pack("s", "'")
        const it = kos.raw_lexer(buf)
        it()
    })
}

do {
    const lexer = lang.buffer().pack("s", ")hello\"") -> kos.raw_lexer

    const t1 = lexer()
    assert t1.line   == 1
    assert t1.column == 1
    assert t1.type   == kos.token_separator
    assert t1.sep    == kos.sep_paren_close
    assert t1.token  == ")"

    const t2 = lexer(kos.continue_string)
    assert t2.line   == 1
    assert t2.column == 1
    assert t2.type   == kos.token_string
    assert t2.token  == ")hello\""

    expect_fail(lexer)
}

do {
    const lexer = lang.buffer().pack("s", ")str\"") -> kos.raw_lexer
    const t     = lexer(kos.continue_string)

    assert t.type  == kos.token_separator
    assert t.sep   == kos.sep_paren_close
    assert t.token == ")"
}

do {
    const lexer = lang.buffer().pack("s", "\"a\\()b\"") -> kos.raw_lexer

    const t1 = lexer()
    assert t1.line   == 1
    assert t1.column == 1
    assert t1.type   == kos.token_string_open
    assert t1.token  == "\"a\\("

    const t2 = lexer(kos.any_token)
    assert t2.line   == 1
    assert t2.column == 5
    assert t2.type   == kos.token_separator
    assert t2.sep    == kos.sep_paren_close
    assert t2.token  == ")"

    const t3 = lexer(kos.continue_string)
    assert t3.line   == 1
    assert t3.column == 5
    assert t3.type   == kos.token_string
    assert t3.token  == ")b\""
}

do {
    const lexer = lang.buffer().pack("s", "(str\"") -> kos.raw_lexer

    const t1 = lexer()
    expect_fail(Î»->(lexer(kos.continue_string)))
}

do {
    const lexer = lang.buffer().pack("s", "{([\"\\()\"])}") -> kos.lexer

    const expected = [ [  1, "token_separator",   "sep_curly_open",   "{"     ],
                       [  2, "token_separator",   "sep_paren_open",   "("     ],
                       [  3, "token_separator",   "sep_square_open",  "["     ],
                       [  4, "token_string_open", "sep_none",         "\"\\(" ], #)\"
                       [  7, "token_string",      "sep_none",         ")\""   ],
                       [  9, "token_separator",   "sep_square_close", "]"     ],
                       [ 10, "token_separator",   "sep_paren_close",  ")"     ],
                       [ 11, "token_separator",   "sep_curly_close",  "}"     ] ]

    var idx = 0

    for var token in lexer {
        assert token.line   == 1
        assert token.column == expected[idx][0]
        assert token.type   == kos[expected[idx][1]]
        assert token.sep    == kos[expected[idx][2]]
        assert token.token  == expected[idx][3]
        idx += 1
    }

    assert idx == 8
}

do {
    const input = "+\\hello"
    const buf   = lang.buffer().pack("s", input)

    do {
        const lexer = kos.raw_lexer(buf)
        const token = lexer()
        assert token.type  == kos.token_operator
        assert token.token == "+"
        assert token.op    == kos.op_add
        expect_fail(lexer)
        expect_fail(lexer)
    }

    const lexer = kos.raw_lexer(buf, true) # ignore_errors=true

    const t1 = lexer()
    assert t1.type   == kos.token_operator
    assert t1.token  == "+"
    assert t1.op     == kos.op_add
    assert t1.line   == 1
    assert t1.column == 1

    const t2 = lexer()
    assert t2.type    == kos.token_whitespace
    assert t2.token   == "\\"
    assert t2.keyword == kos.keyword_none
    assert t2.op      == kos.op_none
    assert t2.sep     == kos.sep_none
    assert t2.line    == 1
    assert t2.column  == 2

    const t3 = lexer()
    assert t3.type   == kos.token_identifier
    assert t3.token  == "hello"
    assert t3.line   == 1
    assert t3.column == 3

    expect_fail(lexer)
}
